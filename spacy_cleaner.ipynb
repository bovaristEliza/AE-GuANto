{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase di pulizia dataset\n",
    "\n",
    "Eseguiamo nuove operazioni di pulizia del dataset, in particolare:\n",
    "- rimozione di caratteri speciali\n",
    "- rimozione di stopwords\n",
    "- rimozione di numeri\n",
    "- lemmatizzazione\n",
    "\n",
    "Gli altri notebook partono dall'output di questo notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#circa 7min di esecuzione con modello già caricato\n",
    "\n",
    "#loading and cleaning dataset with spacy\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "def cleaner(s):\n",
    "    #removing numbers and special caracters\n",
    "    s = re.sub(r'[^a-z\\s]', '', s).strip()\n",
    "\n",
    "    #removing multiple spaces\n",
    "    s = \" \".join(s.split()).strip()\n",
    "\n",
    "    #some manual corrections\n",
    "    s = s.replace(' accino', ' vaccino')\n",
    "\n",
    "    #lemmatization with spacy\n",
    "    doc = nlp(s)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop]).strip()\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    doc = nlp(s)\n",
    "    return \" \".join([token.text for token in doc if not token.is_stop]).strip()\n",
    "\n",
    "'''\n",
    "load_stopwords_list(file_path: str) -> list\n",
    "    Load stopwords from a file containing one stopword per line.\n",
    "'''\n",
    "def load_stopwords_list(file_path = \"data/it_stopwords_kaggle.txt\"):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "#carichiamo il modello\n",
    "nlp = spacy.load(\"it_core_news_md\")\n",
    "\n",
    "#load pandas dataframe\n",
    "vax_df = pd.read_csv('data/posts_cleaned_it_only.csv')\n",
    "\n",
    "#load stopwords and adding to the model\n",
    "italian_stopwords = load_stopwords_list()\n",
    "for stopword in italian_stopwords:\n",
    "    lexeme = nlp.vocab[stopword]\n",
    "    lexeme.is_stop = True\n",
    "\n",
    "\n",
    "#selezioniamo solo la colonna clean_text\n",
    "vax_df = vax_df['clean_text']\n",
    "\n",
    "#applichiamo la funzione di pulizia\n",
    "vax_df_cleaned = vax_df.apply(cleaner)\n",
    "#è necessario applicare nuovamente la funzione per rimuovere le stopwords che sono state aggiunte dalla lemmatizzazione\n",
    "vax_df_cleaned = vax_df_cleaned.apply(remove_stopwords)\n",
    "\n",
    "#esportiamo il dataset pulito in formato csv \n",
    "vax_df_cleaned.to_csv('data/posts_ULTRAcleaned_it_only_spacy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analisi del dataset pulito \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "#contiamo le parole\n",
    "aggregate_counter = Counter()\n",
    "for row in vax_df_cleaned:\n",
    "    c = Counter(row.split())\n",
    "    aggregate_counter += c\n",
    "\n",
    "common_words = [word[0] for word in aggregate_counter.most_common(50)]\n",
    "common_words_counts = [word[1] for word in aggregate_counter.most_common(50)]\n",
    "\n",
    "#disegnamo i grafici\n",
    "def barplot(words, words_counts, title):\n",
    "    fig = plt.figure(figsize=(18,6))\n",
    "    bar_plot = sns.barplot(x=words, y=words_counts)\n",
    "    for item in bar_plot.get_xticklabels():\n",
    "        item.set_rotation(90)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "barplot(words=common_words, words_counts=common_words_counts, title='Most Frequent Words used in novax tweet')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
