{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Anomaly detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization e lematizzazione\n",
    "\n",
    "In questa fase rimuoviamo le stopwords, numeri e caratteri speciali e poniamo tutto in minuscolo. Infine viene effettuata la lemmatizzazione usanto un modello preaddestrato (SpaCy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#circa 7min di esecuzione con modello già caricato\n",
    "\n",
    "#loading and cleaning dataset with spacy\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "def cleaner(s):\n",
    "    #removing numbers and special caracters\n",
    "    s = re.sub(r'[^a-z\\s]', '', s).strip()\n",
    "\n",
    "    #removing multiple spaces\n",
    "    s = \" \".join(s.split()).strip()\n",
    "\n",
    "    #some manual corrections\n",
    "    s = s.replace(' accino', ' vaccino')\n",
    "\n",
    "    #lemmatization with spacy\n",
    "    doc = nlp(s)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop]).strip()\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    doc = nlp(s)\n",
    "    return \" \".join([token.text for token in doc if not token.is_stop]).strip()\n",
    "\n",
    "'''\n",
    "load_stopwords_list(file_path: str) -> list\n",
    "    Load stopwords from a file containing one stopword per line.\n",
    "'''\n",
    "def load_stopwords_list(file_path = \"data/it_stopwords_kaggle.txt\"):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "#carichiamo il modello\n",
    "nlp = spacy.load(\"it_core_news_md\")\n",
    "\n",
    "#load pandas dataframe\n",
    "vax_df = pd.read_csv('data/posts_cleaned_it_only.csv')\n",
    "\n",
    "#load stopwords and adding to the model\n",
    "italian_stopwords = load_stopwords_list()\n",
    "for stopword in italian_stopwords:\n",
    "    lexeme = nlp.vocab[stopword]\n",
    "    lexeme.is_stop = True\n",
    "\n",
    "\n",
    "#selezioniamo solo la colonna clean_text\n",
    "vax_df = vax_df['clean_text']\n",
    "\n",
    "#applichiamo la funzione di pulizia\n",
    "vax_df_cleaned = vax_df.apply(cleaner)\n",
    "#è necessario applicare nuovamente la funzione per rimuovere le stopwords che sono state aggiunte dalla lemmatizzazione\n",
    "vax_df_cleaned = vax_df_cleaned.apply(remove_stopwords)\n",
    "\n",
    "#esportiamo il dataset pulito in formato csv \n",
    "vax_df_cleaned.to_csv('data/posts_ULTRAcleaned_it_only_spacy.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi del dataset  \n",
    "\n",
    "Semplicemente contiamo il numero di parole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vax_df_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m#contiamo le parole\u001b[39;00m\n\u001b[1;32m      7\u001b[0m aggregate_counter \u001b[39m=\u001b[39m Counter()\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m vax_df_cleaned:\n\u001b[1;32m      9\u001b[0m     c \u001b[39m=\u001b[39m Counter(row\u001b[39m.\u001b[39msplit())\n\u001b[1;32m     10\u001b[0m     aggregate_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m c\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vax_df_cleaned' is not defined"
     ]
    }
   ],
   "source": [
    "#analisi del dataset pulito \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "#contiamo le parole\n",
    "aggregate_counter = Counter()\n",
    "for row in vax_df_cleaned:\n",
    "    c = Counter(row.split())\n",
    "    aggregate_counter += c\n",
    "\n",
    "common_words = [word[0] for word in aggregate_counter.most_common(50)]\n",
    "common_words_counts = [word[1] for word in aggregate_counter.most_common(50)]\n",
    "\n",
    "#disegnamo i grafici\n",
    "def barplot(words, words_counts, title):\n",
    "    fig = plt.figure(figsize=(18,6))\n",
    "    bar_plot = sns.barplot(x=words, y=words_counts)\n",
    "    for item in bar_plot.get_xticklabels():\n",
    "        item.set_rotation(90)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "barplot(words=common_words, words_counts=common_words_counts, title='Most Frequent Words used in novax tweet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding  \n",
    "\n",
    "Usiamo un modello preaddestrato (SpaCy) per creare un embedding delle parole."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 4,
>>>>>>> 7a63a796cb219be77212b47fae7d3714e52d37ee
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
      "/tmp/ipykernel_1092/2670759942.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n"
     ]
    }
   ],
   "source": [
    "#vettorizzazione (embedding) del dataset pulito\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "#carichiamo il modello\n",
    "nlp = spacy.load(\"it_core_news_md\")\n",
    "\n",
    "#load pandas dataframe (rimuovo le righe vuote che putroppo sono presenti)\n",
    "vax_series = pd.read_csv('data/posts_ULTRAcleaned_it_only_spacy.csv')\n",
    "vax_series.dropna(inplace=True)\n",
    "\n",
    "#create a new column with the cleaned text dataframe with the vector representation of the text\n",
    "vector_list = []\n",
    "for x in vax_series[\"clean_text\"]:\n",
    "    vector = nlp(x).vector\n",
    "    vector_list.append(vector)\n",
    "\n",
    "#casting series to DataframX\n",
    "vax_df = pd.DataFrame(vax_series)\n",
    "#append vector column\n",
    "vax_df['vector'] = vector_list\n",
    "\n",
    "vector_dim = len(vax_df['vector'][0]) #sono tutte uguali\n",
    "\n",
    "#flatteing (piallare) the vector column in 96 columns\n",
    "vector_dim = len(vax_df['vector'][0]) #sono tutte uguali\n",
    "for i in range(vector_dim):\n",
    "    vax_df[f'vector_{i}'] = vax_df['vector'].apply(lambda x: x[i])\n",
    "\n",
    "#drop the vector column\n",
    "vax_df.drop(columns=['vector'], inplace=True)\n",
    "\n",
    "#esportiamo il dataframe pulito in formato csv \n",
    "vax_df.to_csv('data/post_it_cleaned_vectorized.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning\n",
    "\n",
    "Usiamo un autoencoder per creare un modello che possa riconoscere le anomalie. Prima di tutto improtiamo il dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNSUPervised learning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "NUM_VECTOR_COLUMNS = len(vax_df.columns) -1 #96\n",
    "#settings type for all vector columns \n",
    "#dtypes = {f'vector_{i}': np.float32 for i in range(NUM_VECTOR_COLUMNS)}\n",
    "\n",
    "#load pandas dataframe (rimuovo le righe vuote che putroppo sono presenti)\n",
    "vax_df = pd.read_csv('data/post_it_cleaned_vectorized.csv')\n",
    "vax_df.dropna(inplace=True)\n",
    "\n",
    "#utlili per il processo di addestramento\n",
    "numeric_columns_names = [f'vector_{i}' for i in range(96)]\n",
    "#ORA NON SERVE PIù CONVERTIRE IN ARRAY PERCHè I VALORI SONO GIA' FLOAT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP regressor\n",
    "\n",
    "Usiamo un regressore per capire se una frase è anomala o meno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(hidden_layer_sizes=(192, 47, 192))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(hidden_layer_sizes=(192, 47, 192))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(192, 47, 192))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the autoencoder\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#converto la lista di array numpy in un array numpy\n",
    "#test = vax_df[\"vector\"]\n",
    "\n",
    "auto_encoder = MLPRegressor(hidden_layer_sizes=(\n",
    "                                                 192,\n",
    "                                                 47, \n",
    "                                                 192,\n",
    "                                               ))\n",
    "auto_encoder.fit(vax_df[numeric_columns_names], vax_df[numeric_columns_names])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apsev/.local/lib/python3.8/site-packages/sklearn/base.py:420: UserWarning: X does not have valid feature names, but MLPRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8776479103641818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test delle performance del modello (sempre sui dati di training)\n",
    "predicted_vectors = auto_encoder.predict(vax_df[numeric_columns_names])\n",
    "score = auto_encoder.score(predicted_vectors, vax_df[numeric_columns_names])\n",
    "print(f\"Score: {score}\")\n",
    "\n",
    "pd.DataFrame(auto_encoder.loss_curve_).plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test di inferenza con frasi nuove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9706133843890108, 5.320525383254668]\n"
     ]
    }
   ],
   "source": [
    "# Applicazione del modello all'anomaly detection\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "\n",
    "#loading and cleaning dataset with spacy\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "def cleaner(s):\n",
    "    #removing numbers and special caracters\n",
    "    s = re.sub(r'[^a-z\\s]', '', s).strip()\n",
    "\n",
    "    #removing multiple spaces\n",
    "    s = \" \".join(s.split()).strip()\n",
    "\n",
    "    #some manual corrections\n",
    "    s = s.replace(' accino', ' vaccino')\n",
    "\n",
    "    #lemmatization with spacy\n",
    "    doc = nlp(s)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop]).strip()\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    doc = nlp(s)\n",
    "    return \" \".join([token.text for token in doc if not token.is_stop]).strip()\n",
    "\n",
    "'''\n",
    "load_stopwords_list(file_path: str) -> list\n",
    "    Load stopwords from a file containing one stopword per line.\n",
    "'''\n",
    "def load_stopwords_list(file_path = \"data/it_stopwords_kaggle.txt\"):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "#carichiamo il modello\n",
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "\n",
    "#load stopwords and adding to the model\n",
    "italian_stopwords = load_stopwords_list()\n",
    "for stopword in italian_stopwords:\n",
    "    lexeme = nlp.vocab[stopword]\n",
    "    lexeme.is_stop = True\n",
    "\n",
    "\n",
    "#provo con un testo casuale\n",
    "testo = \"Il cane ha preso la palla\"\n",
    "\n",
    "#cleaning\n",
    "testo = cleaner(testo)\n",
    "\n",
    "#remove stopwords\n",
    "testo = remove_stopwords(testo)\n",
    "\n",
    "#vectorization\n",
    "testo_vector = nlp(testo).vector\n",
    "\n",
    "#salvo il vettore in dataframe pandas\n",
    "testo_df = pd.DataFrame([testo_vector], columns=[f'vector_{i}' for i in range(NUM_VECTOR_COLUMNS)])\n",
    "\n",
    "#prediction\n",
    "vector_predicted = auto_encoder.predict(testo_df)\n",
    "vector_predicted_df = pd.DataFrame(vector_predicted, columns=[f'vector_{i}' for i in range(NUM_VECTOR_COLUMNS)])\n",
    "\n",
    "#distance\n",
    "distance = [ 1 - cosine(vector_predicted[0], testo_vector), euclidean(vector_predicted[0], testo_vector) ]\n",
    "print(distance)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nota una leggerlo abbassamento della precisione in casi di \"anomalie\" ma forse troppo piccolo per essere significativo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proviamo con keras\n",
    "\n",
    "https://www.kaggle.com/code/robinteuwens/anomaly-detection-with-auto-encoders \n",
    "\n",
    "## Configurazione modello \n",
    "\n",
    "Autoencoder e altri parametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 10:19:05.574089: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-28 10:19:05.774569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-28 10:19:05.774624: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-28 10:19:06.688912: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 10:19:06.689031: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 10:19:06.689040: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 96)                9312      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                1552      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 12        \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8)                 40        \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 16)                144       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 96)                1632      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,874\n",
      "Trainable params: 12,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 10:19:08.973019: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-02-28 10:19:08.973180: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-28 10:19:08.973247: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-28 10:19:08.973317: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-28 10:19:08.973357: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-02-28 10:19:08.973396: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-02-28 10:19:08.973433: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-28 10:19:08.973469: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-28 10:19:08.973510: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-02-28 10:19:08.973520: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-02-28 10:19:08.982471: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# data dimensions // hyperparameters \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "input_dim = vax_df.columns.size -1\n",
    "\n",
    "#CONFIG \n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 100\n",
    "\n",
    "# https://keras.io/layers/core/\n",
    "keras_autoencoder = tf.keras.models.Sequential([\n",
    "    \n",
    "    # deconstruct / encode\n",
    "    tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )), \n",
    "    tf.keras.layers.Dense(16, activation='elu'),\n",
    "    tf.keras.layers.Dense(8, activation='elu'),\n",
    "    tf.keras.layers.Dense(4, activation='elu'),\n",
    "\n",
    "    tf.keras.layers.Dense(2, activation='elu'),\n",
    "    \n",
    "    # reconstruction / decode\n",
    "    tf.keras.layers.Dense(4, activation='elu'),\n",
    "    tf.keras.layers.Dense(8, activation='elu'),\n",
    "    tf.keras.layers.Dense(16, activation='elu'),\n",
    "    tf.keras.layers.Dense(input_dim, activation='elu')\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "# current date and time\n",
    "yyyymmddHHMM = datetime.now().strftime('%Y%m%d%H%M')\n",
    "\n",
    "# new folder for a new run\n",
    "log_subdir = f'{yyyymmddHHMM}_batch{BATCH_SIZE}_layers{len(keras_autoencoder.layers)}'\n",
    "\n",
    "# define our early stopping\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "save_model = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='autoencoder_best_weights.hdf5',\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    verbose=0,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "    f'logs/{log_subdir}',\n",
    "    update_freq='batch'\n",
    ")\n",
    "\n",
    "# callbacks argument only takes a list\n",
    "cb = [early_stop, save_model, tensorboard]\n",
    "\n",
    "# https://keras.io/api/models/model_training_apis/\n",
    "keras_autoencoder.compile(optimizer=\"adam\", \n",
    "                    loss=\"mse\",\n",
    "                    metrics=[\"acc\"])\n",
    "\n",
    "# print an overview of our model\n",
    "keras_autoencoder.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addestramento autoencoder (con Keras)  \n",
    "\n",
    "Per non fare confusione con il modello precedente l'ho rinominato in keras_autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 2.4819 - acc: 0.0273WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 1s 4ms/step - loss: 2.4328 - acc: 0.0331\n",
      "Epoch 2/100\n",
      "63/75 [========================>.....] - ETA: 0s - loss: 1.5624 - acc: 0.1037WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 1.5517 - acc: 0.1261\n",
      "Epoch 3/100\n",
      "57/75 [=====================>........] - ETA: 0s - loss: 1.4511 - acc: 0.2520WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 1.4449 - acc: 0.2532\n",
      "Epoch 4/100\n",
      "58/75 [======================>.......] - ETA: 0s - loss: 1.4208 - acc: 0.2540WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 1.4140 - acc: 0.2533\n",
      "Epoch 5/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.3933 - acc: 0.2524WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 1.3929 - acc: 0.2532\n",
      "Epoch 6/100\n",
      "57/75 [=====================>........] - ETA: 0s - loss: 1.3820 - acc: 0.2542WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 1.3778 - acc: 0.2531\n",
      "Epoch 7/100\n",
      "58/75 [======================>.......] - ETA: 0s - loss: 1.3570 - acc: 0.2544WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 1.3617 - acc: 0.2532\n",
      "Epoch 8/100\n",
      "58/75 [======================>.......] - ETA: 0s - loss: 1.3459 - acc: 0.2559WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 1.3418 - acc: 0.2532\n",
      "Epoch 9/100\n",
      "57/75 [=====================>........] - ETA: 0s - loss: 1.3205 - acc: 0.2533WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 1.3184 - acc: 0.2532\n",
      "Epoch 10/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 1.2951 - acc: 0.2539WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 1.2944 - acc: 0.2544\n",
      "Epoch 11/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.2762 - acc: 0.2664WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2746 - acc: 0.2679\n",
      "Epoch 12/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 1.2605 - acc: 0.2854WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2618 - acc: 0.2850\n",
      "Epoch 13/100\n",
      "72/75 [===========================>..] - ETA: 0s - loss: 1.2539 - acc: 0.2891WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2547 - acc: 0.2893\n",
      "Epoch 14/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.2545 - acc: 0.2934WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2508 - acc: 0.2944\n",
      "Epoch 15/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.2498 - acc: 0.2957WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2487 - acc: 0.2951\n",
      "Epoch 16/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.2451 - acc: 0.2941WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2473 - acc: 0.2948\n",
      "Epoch 17/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.2478 - acc: 0.2967WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2466 - acc: 0.2962\n",
      "Epoch 18/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.2490 - acc: 0.2963WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2460 - acc: 0.2964\n",
      "Epoch 19/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.2482 - acc: 0.2938WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2455 - acc: 0.2942\n",
      "Epoch 20/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.2482 - acc: 0.2957WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2449 - acc: 0.2959\n",
      "Epoch 21/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.2418 - acc: 0.2946WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2448 - acc: 0.2954\n",
      "Epoch 22/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.2418 - acc: 0.2970WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2444 - acc: 0.2965\n",
      "Epoch 23/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.2408 - acc: 0.2978WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2439 - acc: 0.2963\n",
      "Epoch 24/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.2395 - acc: 0.2961WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2437 - acc: 0.2965\n",
      "Epoch 25/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.2439 - acc: 0.2947WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2431 - acc: 0.2956\n",
      "Epoch 26/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.2407 - acc: 0.2963WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2428 - acc: 0.2965\n",
      "Epoch 27/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.2433 - acc: 0.2964WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2423 - acc: 0.2969\n",
      "Epoch 28/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.2420 - acc: 0.2980WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2419 - acc: 0.2979\n",
      "Epoch 29/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.2383 - acc: 0.2955WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2415 - acc: 0.2966\n",
      "Epoch 30/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.2433 - acc: 0.2966WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2409 - acc: 0.2975\n",
      "Epoch 31/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.2410 - acc: 0.2978WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2405 - acc: 0.2966\n",
      "Epoch 32/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.2387 - acc: 0.2981WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2398 - acc: 0.2972\n",
      "Epoch 33/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.2380 - acc: 0.2988WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2391 - acc: 0.2974\n",
      "Epoch 34/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.2382 - acc: 0.2963WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2383 - acc: 0.2976\n",
      "Epoch 35/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.2371 - acc: 0.2978WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2368 - acc: 0.2977\n",
      "Epoch 36/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.2316 - acc: 0.2950WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2309 - acc: 0.2959\n",
      "Epoch 37/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.2094 - acc: 0.2944WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2069 - acc: 0.2960\n",
      "Epoch 38/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.1765 - acc: 0.3118WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1758 - acc: 0.3133\n",
      "Epoch 39/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.1653 - acc: 0.3170WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1644 - acc: 0.3171\n",
      "Epoch 40/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1619 - acc: 0.3121WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1603 - acc: 0.3135\n",
      "Epoch 41/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1602 - acc: 0.3098WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1580 - acc: 0.3103\n",
      "Epoch 42/100\n",
      "67/75 [=========================>....] - ETA: 0s - loss: 1.1566 - acc: 0.3112WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1565 - acc: 0.3097\n",
      "Epoch 43/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.1568 - acc: 0.3097WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1555 - acc: 0.3094\n",
      "Epoch 44/100\n",
      "66/75 [=========================>....] - ETA: 0s - loss: 1.1531 - acc: 0.3103WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1548 - acc: 0.3104\n",
      "Epoch 45/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1505 - acc: 0.3119WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1537 - acc: 0.3119\n",
      "Epoch 46/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1503 - acc: 0.3097WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1531 - acc: 0.3105\n",
      "Epoch 47/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1526 - acc: 0.3122WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1524 - acc: 0.3119\n",
      "Epoch 48/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1517 - acc: 0.3127WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1519 - acc: 0.3129\n",
      "Epoch 49/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1510 - acc: 0.3178WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1516 - acc: 0.3162\n",
      "Epoch 50/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1503 - acc: 0.3214WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1507 - acc: 0.3190\n",
      "Epoch 51/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1505 - acc: 0.3207WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1502 - acc: 0.3208\n",
      "Epoch 52/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1489 - acc: 0.3154WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1500 - acc: 0.3168\n",
      "Epoch 53/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1471 - acc: 0.3205WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1493 - acc: 0.3199\n",
      "Epoch 54/100\n",
      "66/75 [=========================>....] - ETA: 0s - loss: 1.1463 - acc: 0.3226WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1488 - acc: 0.3232\n",
      "Epoch 55/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1480 - acc: 0.3233WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1484 - acc: 0.3246\n",
      "Epoch 56/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1451 - acc: 0.3252WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1479 - acc: 0.3246\n",
      "Epoch 57/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.1477 - acc: 0.3263WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1473 - acc: 0.3264\n",
      "Epoch 58/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1475 - acc: 0.3263WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1468 - acc: 0.3260\n",
      "Epoch 59/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.1496 - acc: 0.3261WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1465 - acc: 0.3270\n",
      "Epoch 60/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1414 - acc: 0.3270WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1461 - acc: 0.3264\n",
      "Epoch 61/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1460 - acc: 0.3274WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1451 - acc: 0.3271\n",
      "Epoch 62/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.1429 - acc: 0.3254WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1445 - acc: 0.3261\n",
      "Epoch 63/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.1432 - acc: 0.3283WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1438 - acc: 0.3274\n",
      "Epoch 64/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1460 - acc: 0.3308WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1429 - acc: 0.3281\n",
      "Epoch 65/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1439 - acc: 0.3285WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1419 - acc: 0.3273\n",
      "Epoch 66/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1412 - acc: 0.3246WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1409 - acc: 0.3265\n",
      "Epoch 67/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1421 - acc: 0.3250WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1396 - acc: 0.3247\n",
      "Epoch 68/100\n",
      "66/75 [=========================>....] - ETA: 0s - loss: 1.1424 - acc: 0.3255WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1384 - acc: 0.3245\n",
      "Epoch 69/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1370 - acc: 0.3268WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1368 - acc: 0.3258\n",
      "Epoch 70/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1334 - acc: 0.3246WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1350 - acc: 0.3260\n",
      "Epoch 71/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.1355 - acc: 0.3254WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1335 - acc: 0.3248\n",
      "Epoch 72/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.1327 - acc: 0.3241WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1317 - acc: 0.3238\n",
      "Epoch 73/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1285 - acc: 0.3246WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1297 - acc: 0.3246\n",
      "Epoch 74/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1297 - acc: 0.3243WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1278 - acc: 0.3246\n",
      "Epoch 75/100\n",
      "67/75 [=========================>....] - ETA: 0s - loss: 1.1209 - acc: 0.3219WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1262 - acc: 0.3221\n",
      "Epoch 76/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.1241 - acc: 0.3197WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1240 - acc: 0.3188\n",
      "Epoch 77/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.1209 - acc: 0.3193WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1223 - acc: 0.3188\n",
      "Epoch 78/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1225 - acc: 0.3204WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1207 - acc: 0.3202\n",
      "Epoch 79/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.1217 - acc: 0.3183WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1194 - acc: 0.3192\n",
      "Epoch 80/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1185 - acc: 0.3205WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1181 - acc: 0.3205\n",
      "Epoch 81/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1180 - acc: 0.3170WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1168 - acc: 0.3182\n",
      "Epoch 82/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1143 - acc: 0.3171WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1161 - acc: 0.3168\n",
      "Epoch 83/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1162 - acc: 0.3186WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1149 - acc: 0.3193\n",
      "Epoch 84/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1154 - acc: 0.3192WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1137 - acc: 0.3180\n",
      "Epoch 85/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.1137 - acc: 0.3171WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1126 - acc: 0.3174\n",
      "Epoch 86/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1116 - acc: 0.3187WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1116 - acc: 0.3185\n",
      "Epoch 87/100\n",
      "65/75 [=========================>....] - ETA: 0s - loss: 1.1145 - acc: 0.3153WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1109 - acc: 0.3179\n",
      "Epoch 88/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1100 - acc: 0.3180WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1100 - acc: 0.3189\n",
      "Epoch 89/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1087 - acc: 0.3181WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1094 - acc: 0.3181\n",
      "Epoch 90/100\n",
      "65/75 [=========================>....] - ETA: 0s - loss: 1.1091 - acc: 0.3182WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1088 - acc: 0.3179\n",
      "Epoch 91/100\n",
      "66/75 [=========================>....] - ETA: 0s - loss: 1.1076 - acc: 0.3195WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1083 - acc: 0.3187\n",
      "Epoch 92/100\n",
      "66/75 [=========================>....] - ETA: 0s - loss: 1.1117 - acc: 0.3182WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1079 - acc: 0.3202\n",
      "Epoch 93/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1071 - acc: 0.3192WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1073 - acc: 0.3198\n",
      "Epoch 94/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1071 - acc: 0.3206WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1066 - acc: 0.3210\n",
      "Epoch 95/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.1040 - acc: 0.3190WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1061 - acc: 0.3194\n",
      "Epoch 96/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.1063 - acc: 0.3224WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1056 - acc: 0.3217\n",
      "Epoch 97/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1053 - acc: 0.3166WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1052 - acc: 0.3179\n",
      "Epoch 98/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 1.1048 - acc: 0.3187WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 1.1051 - acc: 0.3194\n",
      "Epoch 99/100\n",
      "66/75 [=========================>....] - ETA: 0s - loss: 1.1021 - acc: 0.3176WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1045 - acc: 0.3182\n",
      "Epoch 100/100\n",
      "66/75 [=========================>....] - ETA: 0s - loss: 1.1048 - acc: 0.3181WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1042 - acc: 0.3184\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = keras_autoencoder.fit(\n",
    "    vax_df[numeric_columns_names], vax_df[numeric_columns_names],\n",
    "    shuffle=True,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=cb,\n",
    "    #validation_data=(X_validate_transformed, X_validate_transformed)\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi della mse del dataset originale\n",
    "\n",
    "Cerchiamo di capire come usare la MSE per capire se una frase è anomala o meno. Nello specifico guardiamo la distribuzione e cerchiamo di stabilire una soglia oltre la quale una frase è anomala.  \n",
    "\n",
    "Notiamo che mettendo la soglia a 2 riusciamo a classificare il 90% del dataset in modo corretto. Però anche in questo caso non possiamo dire nulla di preciso sulle anomalie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597/597 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    19097.000000\n",
      "mean         1.104133\n",
      "std          0.820115\n",
      "min          0.278176\n",
      "50%          0.863751\n",
      "75%          1.294261\n",
      "90%          1.969251\n",
      "95%          2.565478\n",
      "99%          3.948294\n",
      "max         13.570157\n",
      "Name: mse, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ANALISI errore ricostruzione nel dataset originale\n",
    "recontructions = keras_autoencoder.predict(vax_df[numeric_columns_names])\n",
    "\n",
    "mse = np.mean(np.power(vax_df[numeric_columns_names] - recontructions, 2), axis=1)\n",
    "\n",
    "#create a dataframe with 2 columns text and mse\n",
    "test = pd.DataFrame({'clean_text': vax_df[\"clean_text\"], 'mse': mse})\n",
    "\n",
    "#draw an histogram with mse\n",
    "plt.hist(test['mse'], bins=100,  label='mse')\n",
    "plt.show()\n",
    "\n",
    "#some statistics\n",
    "print(test['mse'].describe(percentiles=[0.5, 0.75, 0.9, 0.95, 0.99]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test inferenza con testo nuovo\n",
    "\n",
    "Proviamo a usare il modello e calcoliamo l'errore di ricnostruzione (nelle anomalie dovrebbe essere maggiore dato che sono diverse dal dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "0    0.781829\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# Applicazione del modello all'anomaly detection\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "\n",
    "#loading and cleaning dataset with spacy\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "def cleaner(s):\n",
    "    #removing numbers and special caracters\n",
    "    s = re.sub(r'[^a-z\\s]', '', s).strip()\n",
    "\n",
    "    #removing multiple spaces\n",
    "    s = \" \".join(s.split()).strip()\n",
    "\n",
    "    #some manual corrections\n",
    "    s = s.replace(' accino', ' vaccino')\n",
    "\n",
    "    #lemmatization with spacy\n",
    "    doc = nlp(s)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop]).strip()\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    doc = nlp(s)\n",
    "    return \" \".join([token.text for token in doc if not token.is_stop]).strip()\n",
    "\n",
    "'''\n",
    "load_stopwords_list(file_path: str) -> list\n",
    "    Load stopwords from a file containing one stopword per line.\n",
    "'''\n",
    "def load_stopwords_list(file_path = \"data/it_stopwords_kaggle.txt\"):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "#carichiamo il modello\n",
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "\n",
    "#load stopwords and adding to the model\n",
    "italian_stopwords = load_stopwords_list()\n",
    "for stopword in italian_stopwords:\n",
    "    lexeme = nlp.vocab[stopword]\n",
    "    lexeme.is_stop = True\n",
    "\n",
    "\n",
    "#provo con un testo casuale\n",
    "testo = \"i sono il bene che è sceso da dio in terra grazie alla scienza ed alla medicina possimmo salvare la vita di milioni di persone e curarci\"\n",
    "#testo = \"i vaccini e le vaccinazioni sono dannosi\"\n",
    "\n",
    "#cleaning\n",
    "testo = cleaner(testo)\n",
    "\n",
    "#remove stopwords\n",
    "testo = remove_stopwords(testo)\n",
    "\n",
    "#vectorization\n",
    "testo_vector = nlp(testo).vector\n",
    "\n",
    "#salvo il vettore in dataframe pandas\n",
    "testo_df = pd.DataFrame([testo_vector], columns=[f'vector_{i}' for i in range(NUM_VECTOR_COLUMNS)])\n",
    "\n",
    "#prediction\n",
    "vector_predicted = keras_autoencoder.predict(testo_df)\n",
    "vector_predicted_df = pd.DataFrame(vector_predicted, columns=[f'vector_{i}' for i in range(NUM_VECTOR_COLUMNS)])\n",
    "\n",
    "#distance\n",
    "#distance = [ 1 - cosine(vector_predicted[0], testo_vector), euclidean(vector_predicted[0], testo_vector) ]\n",
    "\n",
    "#reconstruction error with mse\n",
    "distance = np.mean(np.power(testo_df - vector_predicted_df, 2), axis=1)\n",
    "\n",
    "print(distance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Injection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "67/75 [=========================>....] - ETA: 0s - loss: 2.6235 - acc: 0.0527 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 2s 5ms/step - loss: 2.5195 - acc: 0.0540\n",
      "Epoch 2/100\n",
      "73/75 [============================>.] - ETA: 0s - loss: 1.4494 - acc: 0.2511WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.4471 - acc: 0.2511\n",
      "Epoch 3/100\n",
      "66/75 [=========================>....] - ETA: 0s - loss: 1.4002 - acc: 0.2546WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.3974 - acc: 0.2531\n",
      "Epoch 4/100\n",
      "65/75 [=========================>....] - ETA: 0s - loss: 1.3776 - acc: 0.2551WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.3803 - acc: 0.2572\n",
      "Epoch 5/100\n",
      "66/75 [=========================>....] - ETA: 0s - loss: 1.3547 - acc: 0.2646WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.3555 - acc: 0.2669\n",
      "Epoch 6/100\n",
      "66/75 [=========================>....] - ETA: 0s - loss: 1.3080 - acc: 0.2733WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.3097 - acc: 0.2723\n",
      "Epoch 7/100\n",
      "66/75 [=========================>....] - ETA: 0s - loss: 1.2783 - acc: 0.2807WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2720 - acc: 0.2795\n",
      "Epoch 8/100\n",
      "67/75 [=========================>....] - ETA: 0s - loss: 1.2608 - acc: 0.2874WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2616 - acc: 0.2882\n",
      "Epoch 9/100\n",
      "66/75 [=========================>....] - ETA: 0s - loss: 1.2650 - acc: 0.2875WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2588 - acc: 0.2894\n",
      "Epoch 10/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.2629 - acc: 0.2848WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2576 - acc: 0.2865\n",
      "Epoch 11/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.2567 - acc: 0.2909WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2570 - acc: 0.2912\n",
      "Epoch 12/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.2536 - acc: 0.2901WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2562 - acc: 0.2903\n",
      "Epoch 13/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.2580 - acc: 0.2890WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2558 - acc: 0.2896\n",
      "Epoch 14/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.2552 - acc: 0.2887WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2560 - acc: 0.2905\n",
      "Epoch 15/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.2562 - acc: 0.2901WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2553 - acc: 0.2905\n",
      "Epoch 16/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.2533 - acc: 0.2870WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2554 - acc: 0.2882\n",
      "Epoch 17/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.2553 - acc: 0.2896WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2554 - acc: 0.2898\n",
      "Epoch 18/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.2582 - acc: 0.2892WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2549 - acc: 0.2898\n",
      "Epoch 19/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.2556 - acc: 0.2897WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2545 - acc: 0.2900\n",
      "Epoch 20/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.2535 - acc: 0.2903WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2544 - acc: 0.2899\n",
      "Epoch 21/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.2521 - acc: 0.2873WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2541 - acc: 0.2887\n",
      "Epoch 22/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.2558 - acc: 0.2887WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2540 - acc: 0.2887\n",
      "Epoch 23/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.2547 - acc: 0.2893WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2536 - acc: 0.2895\n",
      "Epoch 24/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.2541 - acc: 0.2890WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2532 - acc: 0.2875\n",
      "Epoch 25/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.2493 - acc: 0.2912WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2521 - acc: 0.2902\n",
      "Epoch 26/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.2508 - acc: 0.2905WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2485 - acc: 0.2905\n",
      "Epoch 27/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.2409 - acc: 0.2920WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2408 - acc: 0.2919\n",
      "Epoch 28/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.2368 - acc: 0.2930WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2362 - acc: 0.2960\n",
      "Epoch 29/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.2327 - acc: 0.2949WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2310 - acc: 0.2956\n",
      "Epoch 30/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.2252 - acc: 0.2983WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2229 - acc: 0.2971\n",
      "Epoch 31/100\n",
      "67/75 [=========================>....] - ETA: 0s - loss: 1.2115 - acc: 0.2993WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2132 - acc: 0.2967\n",
      "Epoch 32/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.2040 - acc: 0.2888WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.2023 - acc: 0.2898\n",
      "Epoch 33/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1916 - acc: 0.2896WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1921 - acc: 0.2900\n",
      "Epoch 34/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1852 - acc: 0.2889WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1846 - acc: 0.2888\n",
      "Epoch 35/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1806 - acc: 0.2911WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1789 - acc: 0.2906\n",
      "Epoch 36/100\n",
      "66/75 [=========================>....] - ETA: 0s - loss: 1.1781 - acc: 0.2907WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1761 - acc: 0.2898\n",
      "Epoch 37/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1744 - acc: 0.2925WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1731 - acc: 0.2924\n",
      "Epoch 38/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1696 - acc: 0.2873WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1703 - acc: 0.2874\n",
      "Epoch 39/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1698 - acc: 0.2896WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1685 - acc: 0.2899\n",
      "Epoch 40/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1681 - acc: 0.2879WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1667 - acc: 0.2872\n",
      "Epoch 41/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1672 - acc: 0.2895WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1652 - acc: 0.2897\n",
      "Epoch 42/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1641 - acc: 0.2912WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1635 - acc: 0.2901\n",
      "Epoch 43/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1654 - acc: 0.2881WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1615 - acc: 0.2898\n",
      "Epoch 44/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1581 - acc: 0.2871WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1601 - acc: 0.2875\n",
      "Epoch 45/100\n",
      "66/75 [=========================>....] - ETA: 0s - loss: 1.1573 - acc: 0.2908WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1584 - acc: 0.2909\n",
      "Epoch 46/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1531 - acc: 0.2879WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1566 - acc: 0.2888\n",
      "Epoch 47/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1564 - acc: 0.2937WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1549 - acc: 0.2918\n",
      "Epoch 48/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1514 - acc: 0.2912WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1536 - acc: 0.2919\n",
      "Epoch 49/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1519 - acc: 0.2898WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1524 - acc: 0.2922\n",
      "Epoch 50/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1542 - acc: 0.2917WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1512 - acc: 0.2925\n",
      "Epoch 51/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1532 - acc: 0.2925WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1509 - acc: 0.2915\n",
      "Epoch 52/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1490 - acc: 0.2908WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1496 - acc: 0.2919\n",
      "Epoch 53/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1498 - acc: 0.2921WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1489 - acc: 0.2929\n",
      "Epoch 54/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1470 - acc: 0.2899WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1481 - acc: 0.2890\n",
      "Epoch 55/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1466 - acc: 0.2937WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1475 - acc: 0.2939\n",
      "Epoch 56/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1484 - acc: 0.2944WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1466 - acc: 0.2945\n",
      "Epoch 57/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1430 - acc: 0.2936WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1457 - acc: 0.2925\n",
      "Epoch 58/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1445 - acc: 0.2939WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1453 - acc: 0.2947\n",
      "Epoch 59/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1469 - acc: 0.2964WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1446 - acc: 0.2948\n",
      "Epoch 60/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1468 - acc: 0.2948WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1440 - acc: 0.2952\n",
      "Epoch 61/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1442 - acc: 0.2973WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1436 - acc: 0.2971\n",
      "Epoch 62/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1444 - acc: 0.2934WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1428 - acc: 0.2934\n",
      "Epoch 63/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1400 - acc: 0.2948WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1429 - acc: 0.2957\n",
      "Epoch 64/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1428 - acc: 0.2937WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1420 - acc: 0.2962\n",
      "Epoch 65/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.1441 - acc: 0.2965WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1419 - acc: 0.2962\n",
      "Epoch 66/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1438 - acc: 0.2957WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1416 - acc: 0.2954\n",
      "Epoch 67/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1424 - acc: 0.2927WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1414 - acc: 0.2933\n",
      "Epoch 68/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1414 - acc: 0.2980WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1408 - acc: 0.2975\n",
      "Epoch 69/100\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 1.1391 - acc: 0.2944WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1405 - acc: 0.2937\n",
      "Epoch 70/100\n",
      "71/75 [===========================>..] - ETA: 0s - loss: 1.1408 - acc: 0.2960WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1402 - acc: 0.2961\n",
      "Epoch 71/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1412 - acc: 0.2969WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1401 - acc: 0.2955\n",
      "Epoch 72/100\n",
      "67/75 [=========================>....] - ETA: 0s - loss: 1.1400 - acc: 0.2920WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1401 - acc: 0.2937\n",
      "Epoch 73/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1395 - acc: 0.2925WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1397 - acc: 0.2938\n",
      "Epoch 74/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1393 - acc: 0.2925WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1397 - acc: 0.2943\n",
      "Epoch 75/100\n",
      "66/75 [=========================>....] - ETA: 0s - loss: 1.1422 - acc: 0.2932WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1391 - acc: 0.2947\n",
      "Epoch 76/100\n",
      "66/75 [=========================>....] - ETA: 0s - loss: 1.1407 - acc: 0.2880WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1391 - acc: 0.2908\n",
      "Epoch 77/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1368 - acc: 0.2960WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1384 - acc: 0.2959\n",
      "Epoch 78/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1383 - acc: 0.2994WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1379 - acc: 0.2977\n",
      "Epoch 79/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1355 - acc: 0.2985WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1378 - acc: 0.2977\n",
      "Epoch 80/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1376 - acc: 0.2980WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1386 - acc: 0.2961\n",
      "Epoch 81/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1419 - acc: 0.2981WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1374 - acc: 0.2977\n",
      "Epoch 82/100\n",
      "66/75 [=========================>....] - ETA: 0s - loss: 1.1339 - acc: 0.3007WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1375 - acc: 0.2985\n",
      "Epoch 83/100\n",
      "67/75 [=========================>....] - ETA: 0s - loss: 1.1380 - acc: 0.2940WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1372 - acc: 0.2923\n",
      "Epoch 84/100\n",
      "66/75 [=========================>....] - ETA: 0s - loss: 1.1376 - acc: 0.2972WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1372 - acc: 0.2967\n",
      "Epoch 85/100\n",
      "65/75 [=========================>....] - ETA: 0s - loss: 1.1364 - acc: 0.2976WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1369 - acc: 0.2967\n",
      "Epoch 86/100\n",
      "66/75 [=========================>....] - ETA: 0s - loss: 1.1327 - acc: 0.2988WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1368 - acc: 0.3000\n",
      "Epoch 87/100\n",
      "67/75 [=========================>....] - ETA: 0s - loss: 1.1344 - acc: 0.2958WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1364 - acc: 0.2957\n",
      "Epoch 88/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1369 - acc: 0.2958WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1360 - acc: 0.2982\n",
      "Epoch 89/100\n",
      "66/75 [=========================>....] - ETA: 0s - loss: 1.1343 - acc: 0.3017WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1360 - acc: 0.3012\n",
      "Epoch 90/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1337 - acc: 0.2985WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1359 - acc: 0.2978\n",
      "Epoch 91/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1353 - acc: 0.2960WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1353 - acc: 0.2966\n",
      "Epoch 92/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1340 - acc: 0.2972WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1346 - acc: 0.2983\n",
      "Epoch 93/100\n",
      "67/75 [=========================>....] - ETA: 0s - loss: 1.1362 - acc: 0.2996WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1345 - acc: 0.2999\n",
      "Epoch 94/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1334 - acc: 0.2973WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1343 - acc: 0.2983\n",
      "Epoch 95/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1317 - acc: 0.2981WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1339 - acc: 0.2992\n",
      "Epoch 96/100\n",
      "69/75 [==========================>...] - ETA: 0s - loss: 1.1313 - acc: 0.3016WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1337 - acc: 0.3000\n",
      "Epoch 97/100\n",
      "67/75 [=========================>....] - ETA: 0s - loss: 1.1312 - acc: 0.3015WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1337 - acc: 0.3019\n",
      "Epoch 98/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1355 - acc: 0.2995WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1333 - acc: 0.2997\n",
      "Epoch 99/100\n",
      "68/75 [==========================>...] - ETA: 0s - loss: 1.1350 - acc: 0.2991WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1336 - acc: 0.2998\n",
      "Epoch 100/100\n",
      "67/75 [=========================>....] - ETA: 0s - loss: 1.1323 - acc: 0.3018WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 1.1331 - acc: 0.3001\n"
     ]
    }
   ],
   "source": [
    "def add_noise(x, noise_factor=0.2):\n",
    "    x = x + np.random.randn(*x.shape) * noise_factor\n",
    "    x = x.clip(0., 1.)\n",
    "    return x\n",
    "\n",
    "X_train_noisy = add_noise(vax_df[numeric_columns_names])\n",
    "\n",
    "history = keras_autoencoder.fit(\n",
    "    X_train_noisy, vax_df[numeric_columns_names],\n",
    "    shuffle=True,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=cb,\n",
    "    #validation_data=(X_validate_transformed, X_validate_transformed)\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597/597 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    19097.000000\n",
      "mean         2.697557\n",
      "std          5.590283\n",
      "min          0.430357\n",
      "50%          1.642206\n",
      "75%          2.303154\n",
      "90%          3.901339\n",
      "95%          6.423464\n",
      "99%         26.327825\n",
      "max         93.320524\n",
      "Name: mse, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# ANALISI errore ricostruzione nel dataset originale\n",
    "recontructions = keras_autoencoder.predict(vax_df[numeric_columns_names])\n",
    "\n",
    "mse = np.mean(np.power(vax_df[numeric_columns_names] - recontructions, 2), axis=1)\n",
    "\n",
    "#create a dataframe with 2 columns text and mse\n",
    "test = pd.DataFrame({'clean_text': vax_df[\"clean_text\"], 'mse': mse})\n",
    "\n",
    "#draw an histogram with mse\n",
    "plt.hist(test['mse'], bins=100,  label='mse')\n",
    "plt.show()\n",
    "\n",
    "#some statistics\n",
    "print(test['mse'].describe(percentiles=[0.5, 0.75, 0.9, 0.95, 0.99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "0    2.56914\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# Applicazione del modello all'anomaly detection\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "NUM_VECTOR_COLUMNS = len(vax_df.columns) -1 #96\n",
    "#loading and cleaning dataset with spacy\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "def cleaner(s):\n",
    "    #removing numbers and special caracters\n",
    "    s = re.sub(r'[^a-z\\s]', '', s).strip()\n",
    "\n",
    "    #removing multiple spaces\n",
    "    s = \" \".join(s.split()).strip()\n",
    "\n",
    "    #some manual corrections\n",
    "    s = s.replace(' accino', ' vaccino')\n",
    "\n",
    "    #lemmatization with spacy\n",
    "    doc = nlp(s)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop]).strip()\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    doc = nlp(s)\n",
    "    return \" \".join([token.text for token in doc if not token.is_stop]).strip()\n",
    "\n",
    "'''\n",
    "load_stopwords_list(file_path: str) -> list\n",
    "    Load stopwords from a file containing one stopword per line.\n",
    "'''\n",
    "def load_stopwords_list(file_path = \"data/it_stopwords_kaggle.txt\"):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "#carichiamo il modello\n",
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "\n",
    "#load stopwords and adding to the model\n",
    "italian_stopwords = load_stopwords_list()\n",
    "for stopword in italian_stopwords:\n",
    "    lexeme = nlp.vocab[stopword]\n",
    "    lexeme.is_stop = True\n",
    "\n",
    "\n",
    "#provo con un testo casuale\n",
    "testo = \"i pony sono gli animali più belli del mondo\"\n",
    "#testo = \"i vaccini e le vaccinazioni sono dannosi\"\n",
    "\n",
    "#cleaning\n",
    "testo = cleaner(testo)\n",
    "\n",
    "#remove stopwords\n",
    "testo = remove_stopwords(testo)\n",
    "\n",
    "#vectorization\n",
    "testo_vector = nlp(testo).vector\n",
    "\n",
    "#salvo il vettore in dataframe pandas\n",
    "testo_df = pd.DataFrame([testo_vector], columns=[f'vector_{i}' for i in range(NUM_VECTOR_COLUMNS)])\n",
    "\n",
    "#prediction\n",
    "vector_predicted = keras_autoencoder.predict(testo_df)\n",
    "vector_predicted_df = pd.DataFrame(vector_predicted, columns=[f'vector_{i}' for i in range(NUM_VECTOR_COLUMNS)])\n",
    "\n",
    "#distance\n",
    "#distance = [ 1 - cosine(vector_predicted[0], testo_vector), euclidean(vector_predicted[0], testo_vector) ]\n",
    "\n",
    "#reconstruction error with mse\n",
    "distance = np.mean(np.power(testo_df - vector_predicted_df, 2), axis=1)\n",
    "\n",
    "print(distance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
