{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence embedding 1 using Doc2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Caricamento dataset\n",
    "\n",
    "Carichiamo il dataset ULTRAcleaned che è stato precedentemente pulito da stopwords, numeri e punteggiatura oltre ad essere stato lemmatizzato con Spacy. Per info controllare il notebook spacy_cleaner.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "#load pandas dataframe (rimuovo le righe vuote che putroppo sono presenti)\n",
    "vax_series = pd.read_csv('data/posts_ULTRAcleaned_it_only_spacy.csv')\n",
    "vax_series.dropna(inplace=True)\n",
    "\n",
    "#salviamo una lista di lista con le parole tokenizzate\n",
    "tokenized_sent = []\n",
    "for s in vax_series[\"clean_text\"]:\n",
    "    print(word_tokenize(s))\n",
    "    tokenized_sent.append(word_tokenize(s) )\n",
    "print(tokenized_sent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec training\n",
    "\n",
    "Addestra il modello Doc2Vec con il dataset ULTRAcleaned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvector_size = Dimensionality of the feature vectors.\\nwindow = The maximum distance between the current and predicted word within a sentence.\\nmin_count = Ignores all words with total frequency lower than this.\\nalpha = The initial learning rate.\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import\n",
    "\n",
    "#associamo un tag ad ogni frase (in ID)\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sent)]\n",
    "\n",
    "## Train doc2vec model\n",
    "model = Doc2Vec(vector_size = 200, window = 4, min_count = 2, epochs = 80)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save(\"data/d2v.model\")\n",
    "\n",
    "'''\n",
    "vector_size = Dimensionality of the feature vectors.\n",
    "window = The maximum distance between the current and predicted word within a sentence.\n",
    "min_count = Ignores all words with total frequency lower than this.\n",
    "alpha = The initial learning rate.\n",
    "'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test  \n",
    "\n",
    "Per valutare come funziona la vettorizzazione abbiamo calcolato il centro del cluster (ovvero il vettore media di tutti i vettori ottenuti dal dataset) e calcolato la distanza \"cosine\" tra il centro del cluster e ogni vettore predetto usando frasi nuove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.19492900609778918\n"
     ]
    }
   ],
   "source": [
    "from gensim import similarities\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def cleaner(s):\n",
    "    #removing numbers and special caracters\n",
    "    s = re.sub(r'[^a-z\\s]', '', s).strip()\n",
    "\n",
    "    #removing multiple spaces\n",
    "    s = \" \".join(s.split()).strip()\n",
    "\n",
    "    #some manual corrections\n",
    "    s = s.replace(' accino', ' vaccino')\n",
    "\n",
    "    #lemmatization with spacy\n",
    "    doc = nlp(s)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop]).strip()\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    doc = nlp(s)\n",
    "    return \" \".join([token.text for token in doc if not token.is_stop]).strip()\n",
    "def load_stopwords_list(file_path = \"data/it_stopwords_kaggle.txt\"):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "#carichiamo il modello (cleaner)\n",
    "nlp = spacy.load(\"it_core_news_md\")\n",
    "\n",
    "#carico il modello per fare inferenza\n",
    "model = Doc2Vec.load(\"data/d2v.model\")\n",
    "\n",
    "#load stopwords and adding to the model\n",
    "italian_stopwords = load_stopwords_list()\n",
    "for stopword in italian_stopwords:\n",
    "    lexeme = nlp.vocab[stopword]\n",
    "    lexeme.is_stop = True\n",
    "\n",
    "#calculating the center of the cluster\n",
    "#create a numpy array with all the vectors\n",
    "vectors = np.zeros((len(model.dv), 200))\n",
    "for i in range(len(model.dv)):\n",
    "    vectors[i] = model.dv[i]\n",
    "\n",
    "#calculating the mean\n",
    "mean_vector = np.mean(vectors, axis=0)\n",
    "mean_vector\n",
    "\n",
    "#provo con un testo casuale\n",
    "testo = \"i vaccini fanno morire le persone\"\n",
    "testo = \"i gatti giocano a palla in mare\"\n",
    "\n",
    "#cleaning\n",
    "testo = cleaner(testo)\n",
    "\n",
    "#remove stopwords\n",
    "testo = remove_stopwords(testo)\n",
    "\n",
    "\n",
    "test_vector = model.infer_vector(word_tokenize(testo))\n",
    "\n",
    "#calcolo la similarità tra il test e tutti i documenti e ne faccio la media\n",
    "'''dis_list = []\n",
    "for i in range(len(model.docvecs)):\n",
    "    #print(1 - cosine(i, test_vector))\n",
    "    dis =  euclidean(model.docvecs[i], test_vector)\n",
    "    dis = dis + cosine(model.docvecs[i], test_vector)\n",
    "    dis_list.append(dis)'''\n",
    "\n",
    "dis = 1 - cosine(test_vector,mean_vector)\n",
    "\n",
    "\n",
    "print(\"Similarity score:\",dis)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
